ClaimGraph: Designing a Claim-Verification Knowledge Graph System
1. LLM Prompt and Schema for Claim Extraction
Structured JSON Schema: We propose extracting claims from papers into a structured JSON format for easy ingestion into the knowledge graph. Each claim can be represented as an object with fields like text (the claim statement), claim_type, and evidence_type. For example:
{
  "text": "The new material can withstand pressures up to 5 GPa without fracturing, indicating its superior strength compared to traditional alloys.",
  "claim_type": "comparative",
  "evidence_type": "citation"
}
In this schema, claim_type is one of a fixed set (e.g. causal – a cause-effect claim, correlational – an association is noted, comparative – two methods/objects are compared, methodological – a claim about a method or approach, existence – asserting existence of a phenomenon). Likewise, evidence_type denotes how the paper supports the claim: e.g. citation (supported by a referenced source), figure (supported by a figure or diagram), experiment (backed by the authors’ experimental results), other_claim (supported by another claim in the text), general_knowledge (a statement based on well-known background knowledge), or unsupported (no clear support provided). This JSON-centric format enforces consistency and is easy for the graph system to parse. (Notably, research on prompt formats finds that JSON prompts yield high accuracy for complex structured outputs[1], making them well-suited for this task.)
LLM Prompt Design: To reliably extract claims and classifications with GPT-4, we recommend a descriptive system or user prompt that explicitly defines the JSON structure and categories. A zero-shot approach can work by instructing: “Extract all notable claims from the following scientific text. For each claim, output a JSON object with text, claim_type (causal, correlational, comparative, methodological, or existence), and evidence_type (citation, figure, experiment, other_claim, general_knowledge, unsupported). Provide an array of such objects. Only output JSON.” This clear instruction primes the model to produce the desired format. However, a few-shot prompt can further improve reliability: provide one or two examples of input text and the expected JSON output. For instance:
**Input:** In this study, we demonstrate the existence of a previously unknown quantum state that emerges at low temperatures (no prior literature reports this).  
**Output:** [{"text": "A previously unknown quantum state emerges at low temperatures.", "claim_type": "existence", "evidence_type": "experiment"}]
**Input:** The experiment shows that our catalyst speeds up the reaction 2x faster than the standard catalyst [42].  
**Output:** [{"text": "Our catalyst speeds up the reaction twice as much as the standard catalyst.", "claim_type": "comparative", "evidence_type": "citation"}]
By including such examples in the prompt (or in a system message as guidelines), GPT-4 can learn the pattern. The model should be instructed to only output well-formed JSON and to include all detected claims in an array. To handle multiple claims, the final answer would be a JSON array [...] containing one object per claim. Additionally, the prompt can clarify the definitions of claim types (e.g. “causal = one variable directly affects another, correlational = two things vary together without implied causation,” etc.) and evidence types. This ensures the model interprets the categories correctly. With GPT-4’s strong few-shot ability, such a prompt should yield a high-quality structured output. Any ambiguous cases (e.g. a claim that could be both causal and comparative) can be resolved by whichever category the definitions emphasize. In summary, a concise system prompt defining the JSON fields and category options, plus one or two example Q&A pairs, is an optimal strategy for claim extraction with GPT-4. This leverages the model’s capacity to follow format instructions while preserving accuracy in identifying claim content.
(For instance, one can instruct GPT-4: “You are an assistant that extracts scientific claims. Output JSON only, with each claim’s text and classifications. Do not include explanatory text.” The robustness of JSON formatting in LLMs is well-documented, and GPT-4 in particular adheres to such structured output when prompted clearly[2].)
2. Graph Storage and Frontend Visualization Tools
 
Example of a knowledge graph visualization (Cytoscape.js): nodes are colored by type and can be interactively explored.
Graph Database/Storage: For a hackathon-scale project, simplicity and quick integration are key. You can choose between using a lightweight graph database or an in-memory data structure. A popular option is Neo4j, a property graph DB that would allow you to store Papers, Claims, and Evidence as nodes with relationships (e.g. Paper → contains → Claim; Claim → supported_by → Evidence nodes). Neo4j offers a convenient query language (Cypher) and even a built-in visualization tool (Neo4j Bloom) for exploring the graph[3]. However, setting up Neo4j and its integration (e.g. via Neo4j’s REST API or Bolt driver) might be overhead for a hackathon unless you’re already comfortable with it. An alternative is to use a simpler JSON-based store: for example, maintain the graph as a JSON object (or a set of CSV/JSON files) where each node and edge is listed, and load that into the frontend directly. Given the likely modest size of the data in a demo, an in-memory representation (using JavaScript objects or a lightweight library) would be fast and avoid additional backend complexity. If you prefer a database without much setup, you could also consider SQLite with a graph extension or ArangoDB (multi-model DB with graph support), but those might not be necessary for a prototype. In summary, if graph querying needs are simple (e.g., retrieve all claims of a paper, or all evidence for a claim), storing the relationships in application memory or a simple document store will suffice for speed. If you want to demonstrate a more “real” graph backend, Neo4j (with its free tier or local instance) is a strong choice due to rich ecosystem and visualizers.
Visualization Libraries (React/Next.js): The frontend is where an interactive graph can really impress the audience, so using a dedicated graph visualization library is recommended. In a React/Next.js context, several libraries stand out:
•	Cytoscape.js – A robust open-source library for network visualization that can integrate with React (via a wrapper like react-cytoscapejs). Cytoscape.js supports a large number of nodes (100k+ nodes is feasible[4]), many layout algorithms (force-directed, concentric, etc.), and highly customizable styling. It would let you color nodes by type (e.g. paper nodes in one color, claim nodes in another, evidence in another) and apply different shapes or icons for each category. It also supports interactive features like panning, zooming, tooltips, and even filtering/hiding elements via its API. For example, you can programmatically highlight suspicious nodes (set their color to red) or filter nodes by type using Cytoscape’s selectors. The downside is that Cytoscape.js has a moderate learning curve (you’ll need to configure elements and styles in a specific way), but it’s well-documented and powerful[5]. If the team is comfortable with a bit of JS configuration, Cytoscape.js could be an excellent choice for a polished result.
•	React-Force-Graph (d3-force) – This is a React component by Vasco Asturiano (react-force-graph) that provides a WebGL-accelerated force-directed graph visualization out-of-the-box. It’s very hackathon-friendly: you feed it an array of nodes (with IDs and any attributes like group/type) and an array of edges, and it handles the rest. It supports both 2D and 3D displays. You can easily map node types to colors (e.g., pass a function for node canvas drawing or use the library’s property for node color). It also has interaction handlers (click, hover) so you could, for example, display details on click or highlight connected nodes. Filtering can be implemented by simply not rendering certain nodes/links (for example, maintain an internal state of which node types to show). This library shines for quick setup and can handle reasonably large graphs (thousands of nodes) smoothly thanks to WebGL. If your graph is not huge, it will appear very responsive. The look-and-feel is less “rigid” than Cytoscape (nodes float around unless you freeze the simulation), but it’s engaging for users.
•	Vis.js (Vis Network) – A classic JS network visualization library that remains easy to use. There’s a React wrapper (react-vis-network-graph) or you can include it in Next.js directly. With Vis.js, you can create nodes and edges and the library will handle layout and interaction. It supports neat features like clustering and dynamic filtering by node properties. For example, you can filter nodes with just a few lines of code[6] – Vis allows you to hide or show nodes based on a filter function, which could implement a “search” or a “filter by type” control. It also supports configuring physics (layout), and you can style nodes by groups (setting a color per group/type). Vis.js might require some manual DOM handling in React (since it’s not natively React-based), but many have used it successfully in React apps. It’s well-suited for a few hundred nodes (performance can drop with very large networks). The benefit for a hackathon is the low code needed to get a decent visualization: for instance, creating a network can be as simple as: new vis.Network(container, {nodes: [...], edges: [...]}, options), and options can include different colors or shapes[7][6].
•	Sigma.js – Another library aimed at rendering large graphs efficiently (using WebGL). A React version (Sigma React) exists[8]. Sigma is great if you expect the graph to be quite large (tens of thousands of nodes) and need performance. It has built-in support for different node styles and can easily color nodes or scale them. The interactivity (zoom, drag) is smooth. However, customizing Sigma (like custom shapes or complex behaviors) can be a bit more involved, and the React integration might not be as straightforward as React-Force-Graph. For a hack demo, Sigma would be ideal if performance becomes an issue with other libraries.
•	Graphin (AntV G6) – Graphin is a React-oriented graph visualization toolkit built on AntV’s G6 library. It’s essentially an “out-of-the-box” solution: you get a <Graphin> component where you can pass data and configuration, and it provides default styles and interactions. Graphin supports component plugins like toolbars, mini-maps, and context menus which could be very handy (e.g., a toolbar button to filter node types, or a mini-map for overview). Node styling and layouts are available through configuration. This could accelerate development since a lot of functionality comes built-in (e.g., force layouts, click behaviors). The learning curve is moderate (you’d need to read Graphin’s docs), but since it’s designed for React, it meshes well with Next.js. Graphin is less famous than Cytoscape but could give a professional look quickly. It’s worth considering if you want a modern React-first library. (Under the hood it uses G6 which is quite powerful).
Recommendations: Given the time constraints, you likely want the solution that requires the least custom code for a nice result. If your team is not already familiar with one of these, React-Force-Graph is a great default due to its simplicity – you can get a basic graph up in minutes, then incrementally add custom styling. If you need more fine-grained control or features like discrete filtering UI, Cytoscape.js or Graphin would be the next choices (with Cytoscape being very powerful for graph analytics and Graphin providing quick UI components). For node coloring, all these libraries let you assign colors easily (either via a property or a callback). For example, in Cytoscape you can define a style like node[type = "claim"] { background-color: #ff6666; } to color all claim nodes. In React-Force-Graph, you might give nodes a group or type attribute and then use nodeCanvasObject to draw them differently based on that. To mark suspicious nodes (from the bullshit detector in part 5), you can simply set their color to red or add an outline – the library’s API will handle re-rendering when data updates. Basic search can be achieved by, say, on a search query, finding the matching node in the data and programmatically focusing it (e.g., zooming to that node or highlighting it). Libraries like Cytoscape have built-in functions to find a node by ID and zoom to it, and Vis.js also can focus on a node by ID.
In summary, use a JavaScript graph library with React integration to render the nodes/edges, and differentiate nodes by type with color/shape. Both Cytoscape.js and Vis.js support filtering nodes (hiding/showing) with minimal code[6], which meets the requirement for basic filtering/search in the UI. If you store your graph data as JSON, you can easily feed it into these libraries. The combination of a simple storage (JSON/Neo4j) and a proven graph visualization library will allow interactive exploration of the knowledge graph with minimal setup time. The visual emphasis (coloring by node type, marking suspect nodes) can be achieved through configuration or a few lines of style code in all the mentioned tools.
3. Comparing Scientific Literature APIs (Semantic Scholar, arXiv, PubMed/PMC, OpenAlex)
We evaluate each API on four criteria: (a) Full-text PDF access, (b) Citation graph metadata, (c) Ease of integration (REST API), and (d) Rate limits.
•	Semantic Scholar API: This API (often called the Semantic Scholar “Graph” API) provides rich metadata on papers, including titles, authors, abstracts, and citation relationships. However, it does not generally provide full-text PDFs directly – it may include an openAccessPdf link if Semantic Scholar has identified one (such as PDF links for open-access papers), but it’s not a reliable source of full PDFs for all papers. On citation metadata, Semantic Scholar excels: it returns each paper’s references and citations (up to 10k citations per paper via their API)[9], and even citation contexts and intents for some papers (e.g. whether a citation was used as background or support, although that coverage is limited to papers where they have full text). Integration is via a modern REST API (JSON responses). It’s fairly straightforward: you can query by paper ID (Semantic Scholar’s own IDs, DOI, arXiv ID, MAG ID, etc.) and get a JSON object. No official client library is needed; a simple HTTP GET with the appropriate endpoint and API key (if using one) works. One thing to note is rate limiting: without an API key, you’re limited to ~100 requests per 5 minutes[10] (and this is a shared global pool). With a free API key (which academic projects can request), the limit increases significantly (the documentation mentions 1000 requests per second in theory[11], but in practice keys often allow on the order of 100k/day or similar). For a hackathon demo where you might only need to fetch tens or hundreds of papers, the free tier without a key might suffice, but it’s easy to obtain a key to be safe. Overall, Semantic Scholar’s strengths are its comprehensive citation graph and metadata. Its weakness is lack of direct full-text – you’d use it to get paper metadata and then might have to fetch PDFs from elsewhere (or just use abstracts for claim extraction).
•	arXiv API: arXiv is an open preprint repository, so full-text PDF access is excellent – any paper on arXiv can be downloaded via a stable URL (e.g. https://arxiv.org/pdf/{id}.pdf). The arXiv API, however, primarily provides metadata (title, authors, abstract, categories, etc.) via an Atom feed interface. It does not provide citation reference lists or citation counts, since arXiv doesn’t maintain citation metadata. So for citation graph, arXiv’s API offers basically nothing (it’s just papers in isolation). Ease of integration: the API is a bit old-school (XML-based, since it’s basically an RSS/Atom feed or an OAI-PMH service). You can query by search terms, author, etc., and get an XML of results. Parsing that in a Node/JavaScript environment would require either an XML parser or using a community library. However, for hackathon purposes, one might skip the API and simply use arXiv’s open data dump or direct arXiv links if you already know the arXiv IDs of interest. The integration is not as plug-and-play as a modern JSON REST service. Rate limits: arXiv requests that users keep to one request every 3 seconds (roughly 0.3 Hz) to avoid overloading[12]. They allow short bursts (up to ~4 requests/sec for a few seconds) but generally expect low sustained rates. This is relatively restrictive. If you need many arXiv papers, it might be slow unless you batch queries. In summary, the best aspect of arXiv is open access to PDFs and fairly reliable metadata for preprints (especially in CS, math, physics). The downsides are no citation data and a slightly cumbersome API format. For a hackathon, arXiv is ideal if your system is focusing on papers from those fields and you want full text quickly – you could search arXiv for papers and fetch their PDF for claim extraction. But if your scope is broader, arXiv will cover only a subset of literature.
•	PubMed / PMC (PubMed Central): PubMed is a massive index of biomedical literature (mainly abstracts for articles, not full texts), while PMC is a repository of open-access full texts for some of those articles. Through NCBI’s E-utilities (their API), you can access both. Full-text access: Regular PubMed records only have abstracts, not PDFs. However, if an article is in PMC, you can fetch the full text (often in XML or sometimes PDF) from the PMC OAI service or via E-utilities (PMC articles have a PMCID identifier). In practice, one strategy is: use E-utilities to find papers (by keywords or IDs), then check if they have a PMC entry for full text. This is more involved than other APIs. Citation metadata: PubMed’s API does not return reference lists or citation counts. It’s primarily for lookup of articles by ID or search queries to get metadata. Some PMC XMLs contain reference lists, but you’d have to parse those yourself; there’s no unified citation graph API from NCBI for PubMed. Ease of integration: NCBI APIs are robust but a bit archaic – you make HTTP GET requests to endpoints like https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=... and parse XML (though newer versions can return JSON for some calls). There are rate limits and API keys as well. Learning to use these endpoints might take some time if you haven’t before, though many libraries (in Python, JavaScript, etc.) exist to help. For a Next.js app, you might call a serverless function or backend script to query PubMed, since doing it client-side is not ideal (CORS issues and needing an API key). Rate limits: Without an API key, NCBI allows ~3 requests per second[13] from a given IP. With a free API key (easy to obtain), you get up to 10 requests/sec[13]. This is fairly generous; you likely won’t hit it in a demo unless you are batch-fetching dozens of articles at once. One advantage of PubMed is its search – you can find papers by topic easily. But since our use-case is more about verifying specific claims/papers, a search may be less relevant than direct lookup by ID. In summary, PubMed/PMC is best if you need biomedical papers and possibly open full texts, but integration is a bit heavier. For a hackathon, unless your project is bio-focused, you might not choose PubMed first because other APIs (OpenAlex/SemanticScholar) cover metadata more broadly with less fuss.
•	OpenAlex: This is a relatively new, fully open index of the research literature. Full-text access: OpenAlex does not provide full PDFs or full-text content via the API (it’s primarily metadata). It does, however, often include links or pointers if an article is open-access. For example, OpenAlex records have a primary_location field which can include a URL or DOI for the PDF, and an open_access flag indicating if it’s free to read. So you might use OpenAlex to get a PDF URL (via an unpaywall link) for some articles, but generally it’s not a full-text retrieval service[14] (the FAQ notes it doesn’t provide raw full text). Citation graph: OpenAlex’s major strength is that it aggregates and exposes citation data. Each Work entry in OpenAlex lists its references (as OpenAlex IDs or DOIs of the works it cites) and also lists the works that cite it (or at least provides a citation count and a way to query citing works). Essentially, it’s an open replacement for Microsoft Academic Graph and similar databases. This means you can use OpenAlex to traverse the citation network or fetch all references of a given paper easily. Ease of integration: The OpenAlex API is very easy to use – it’s a modern RESTful JSON API with straightforward routes. For example, to get a paper by DOI you can call GET https://api.openalex.org/works/doi:{DOI} and you’ll get a JSON with all metadata (title, authors, etc., plus references list, concepts, etc.). It’s well-documented and doesn’t require an API key for most uses. The responses include lots of detail, and you can apply filters and queries (like searching by title or author). OpenAlex is also very flexible for queries (you can search works by keywords, filter by publication year, etc., if needed). Rate limits: OpenAlex offers 100,000 API calls per day per user with no auth token needed[15]. This is extremely high – effectively ~1.16 requests per second sustained all day, or in bursts even more, as long as you stay under the daily cap. For a hackathon demo, it’s practically unlimited (you won’t reach 100k calls in a short project). They do recommend including your email in queries for identification and to help with caching[15], but even without that it’s fine. This generous limit and ease of access make OpenAlex very attractive. Additionally, OpenAlex data coverage is broad (hundreds of millions of papers across all fields[16][17]).
Best Choice for Hackathon: OpenAlex emerges as the top recommendation for a claim-verification graph, primarily because it provides both the citation network and is easy to integrate. You can quickly look up a paper, get its references (to validate if a cited source exists and gather metadata), and check its status (OpenAlex even flags if a work is retracted – more on that later)[18]. The high rate limit means you won’t worry about hitting quotas during development or demo. Semantic Scholar is also excellent, and might have an edge in certain aspects (for example, it sometimes provides citation context or additional fields like fields of study, citation intent, etc.). But for a fast project, OpenAlex’s no-hassle API and comprehensive data make it ideal. One strategy could be to use OpenAlex as the primary backend for paper metadata and citation links, and supplement with Semantic Scholar if you need something like citation context or search by text.
If your domain is restricted: for instance, if all claims you care about are in CS and on arXiv, then using the arXiv API (or arXiv bulk download) plus Semantic Scholar (for citations) might work. Similarly, for biomedical papers, PubMed plus OpenAlex could be combined (PubMed to get the latest data, OpenAlex for references). But in general, OpenAlex covers arXiv and PubMed content anyway (since it aggregates CrossRef, MAG, etc.), so it can serve as a one-stop source for metadata and citation relations. Therefore, our recommendation is to use OpenAlex for the citation graph and metadata, possibly Semantic Scholar or direct PDF links for full-text if needed, and consider domain-specific APIs only if your project has a special focus that demands them. This way, you prioritize speed of implementation and breadth of coverage, ensuring your system can retrieve and cross-link papers on the fly during claim verification.
4. Fast Citation Validation Pipeline Design
To automatically vet the citations supporting a claim, we outline a pipeline of checks. The goal is to catch problems like a missing reference, a irrelevant citation, or a suspicious citation (retracted or part of a citation loop). The pipeline prioritizes simple methods and readily available data/LLM capabilities so it can be built in 1–2 days:
Step 1: Citation Existence Check – “Does the cited source actually exist and correspond to what is claimed?” The system should first verify that each cited reference is a real, identifiable publication. This can be done by extracting a stable identifier from the citation. Often, scientific papers cite sources by DOI or arXiv ID; if not, a combination of title and author/year can be used. Using an API like CrossRef or OpenAlex, the system attempts to find a match for the reference. For example, if a claim cites “Smith et al. 2020”, the system can call CrossRef’s works endpoint with query “Smith 2020 [title]” or use OpenAlex’s search by citation DOI. If using OpenAlex and the citing paper’s metadata is available, you might already have the DOI of each reference from OpenAlex’s reference list. In that case, simply try to fetch each reference via OpenAlex (e.g., GET /works/{ref_id}) – if it returns a record, the citation is valid (exists in the database). If a reference is not found in these scholarly indexes, it’s a red flag (it could be an invented or incorrect citation). The output of this step is a flag for each citation: found or not found. (Almost all legitimate references should be found in CrossRef/OpenAlex/Semantic Scholar, so a “not found” is suspicious by itself.)
Step 2: Relevance Check (Claim vs Citation) – “Does the cited paper actually support the claim it’s attached to?” This is the hardest part to automate, but can be approximated with LLMs or simple text analysis. A fast approach: use the OpenAI API (GPT-4 or GPT-3.5) to compare the claim text and information from the cited paper. Concretely, once you have identified the cited paper (from step 1), retrieve whatever content is readily available – ideally the abstract or conclusion of that paper. Many APIs can provide an abstract: OpenAlex gives a truncated abstract (as an inverted index you can reconstruct), Semantic Scholar often provides the abstract text, or you might get it via CrossRef. With the claim and the cited paper’s abstract in hand, you can prompt an LLM: “Claim: {X}. Source Abstract: {Y}. Question: Does the content of the source appear to support the claim, based on the abstract?” GPT-4 can then answer yes/no or a short analysis. This can be done for each citation quickly (abstracts are short, so token-wise this is feasible). Alternatively, use embeddings: embed the claim and the cited abstract using OpenAI’s embedding model and compute cosine similarity – a very low similarity might indicate irrelevance. But a high similarity isn’t a guarantee of support, so an LLM judgement is more direct. For a hackathon, using GPT-4 (or even GPT-3.5 for cost) in a loop for each claim-citation pair is acceptable given typically a limited number of claims will be checked live. The LLM can output a judgment like “supported” / “not supported” or a score. We should also handle the case of citation not accessible: if we cannot get any text from the cited source (e.g., behind paywall and no summary available), we may have to skip or mark it as “unable to verify directly”. In some cases, the context in the original paper around the citation might hint at the relevance – but extracting and using that context reliably is complex. A simpler method in the pipeline is: if multiple citations are attached to a claim, assume at least one is meant to support it; you could just check one by one. The output of this step is a label per citation: relevant/supports claim or possibly irrelevant. If an LLM is used, it can also detect if the cited paper contradicts the claim (which would be even worse), but that’s a bonus.
Step 3: Retraction Status Check – “Is the cited paper retracted or known to be invalidated?” This is straightforward using existing datasets. For example, OpenAlex marks retracted works with an is_retracted flag[18], based on the Retraction Watch database. If you have the OpenAlex ID or DOI of the reference, you can query OpenAlex and look at is_retracted in the response. Similarly, CrossRef metadata sometimes lists retraction notices. A quick method: use CrossRef’s API to see if there’s an update entry of type “retraction” for the DOI (CrossRef links papers to updates like retractions or corrections). But leveraging OpenAlex is simplest: e.g., GET /works/doi:... and check for "is_retracted": true. If you have many references to check, OpenAlex allows filtering: you could send one query for multiple DOIs (not directly in one call, but the daily limit is high enough to loop). Another dataset is the Retraction Watch API or dataset – since OpenAlex integrates that, it might not be necessary separately. For a hack, focusing on OpenAlex is fine. Mark any citation that is found to be retracted. It’s worth also flagging if the claim itself (from the original paper) cites a retracted work, as that undermines the support. The output here: a boolean flag for each citation: retracted or not. (We likely will surface this in the graph UI by coloring edges or nodes distinctly if a citation is retracted.)
Step 4: Circular Citation / Reference Integrity Check – “Are we dealing with circular or recursive citations (e.g., sources that cite each other or self-citations that are fishy)?” Circular referencing means Paper A cites Paper B as support, but Paper B (directly or indirectly) cites Paper A back, or a small group of papers cite each other in a loop. This can indicate a citation ring or at least that the evidence is not independent. To detect a direct loop: for each cited paper (from the claim’s perspective), check if that paper’s references include the original paper. In practice, using OpenAlex again: if original paper has ID X and it cites Y, fetch Y’s references (OpenAlex can return a list of referenced IDs for Y) and see if X is in that list. If yes, that’s a direct circular citation (A cites B and B cites A). That’s quite suspicious in a claim-evidence context because it suggests the two papers may be boosting each other without external validation[19]. Another pattern to watch is excessive self-citation: if the claim’s evidence is just the authors citing their own prior work repeatedly, it might be biased or not truly independent evidence. This can be checked by comparing author lists (if the authors of the cited paper overlap heavily with the current paper’s authors, flag it as self-citation). Due to time constraints, the simplest implementation is the direct loop check described. If more time/data is available, one could detect broader citation cartels (clusters of papers that cite only each other), but that likely exceeds hackathon scope. So, for each citation, output a flag: circular_ref = true/false (true if a direct back-citation is found, or if it’s basically a self-citation – you can decide what to treat as “circular” evidence).
After these steps, the pipeline will have augmented each claim’s citation with meta-information: existence (valid/invalid), relevance (supports or not), retraction status, and circularity. With these, you can then aggregate a verdict for the claim: e.g., “Claim X is supported by 3 citations: 2 are valid and relevant, 1 is irrelevant, and 1 of the 3 is from a retracted paper. Confidence in claim = low.” This can feed into your knowledge graph by annotating edges (claim–evidence relationships) with a “validated” or “suspect” property.
Implementation considerations: All these checks leverage external data or quick LLM calls. They are doable in 1–2 days: - Existence via OpenAlex/CrossRef is just REST queries (these sources are up 24/7 and easy to call). - Relevance via OpenAI API: writing a prompt for GPT-3.5/4 to evaluate support is straightforward; the main time is in retrieving the abstract, which again OpenAlex or Semantic Scholar can do for many papers. If abstracts are unavailable, even just the title of the cited paper compared to the claim might be fed to GPT for a very rough guess (GPT-4 can sometimes infer from a title if it seems unrelated, but abstract is much better). This step might not be perfect, but it’s better than nothing and can be iterated quickly. - Retraction via OpenAlex is trivial once you have the IDs. - Circularity via OpenAlex: one additional query per citation to get its refs (or you might have cached the network if you fetched a bunch of related papers already).
This pipeline prioritizes speed and using existing datasets (CrossRef, OpenAlex, etc.) over trying to parse PDFs from scratch. By leaning on those APIs and GPT for judgement calls, you can avoid training new models. Each part is modular, so if something fails (say you can’t get an abstract for a citation), you can skip relevance check for that one and still do the other checks. In practice, these automated checks will quickly highlight issues like: “Citation [42] is referenced in the claim but doesn’t appear in any database” (maybe a fake citation), or “The claim cites [Smith 2020], which is a retracted paper[18],” or “The cited study’s abstract makes no mention of the phenomenon claimed – possible mis-citation.” Such information can be surfaced in the UI (maybe as warning icons on edges). Overall, this pipeline would significantly boost the system’s ability to catch unsupported or deceptive claims by scrutinizing their cited evidence with minimal human intervention.
5. Minimum Viable “Bullshit Detector” for Papers
To protect the knowledge graph from fraudulent or AI-generated content, we propose a lightweight module that flags suspicious papers (and their claims) using a combination of heuristic cues and LLM analysis. The focus is on quick wins – things we can implement fast – rather than an exhaustive deepfake detector. Key components of the BS detector: linguistic cues, citation integrity, and metadata consistency.
Linguistic Cues: Fake or AI-generated papers often have telltale oddities in their writing. One famous indicator is the presence of “tortured phrases” – bizarre paraphrases of standard scientific terms[20]. For example, an AI-generated paper might use phrases like “counterfeit consciousness” instead of “artificial intelligence” or “colossal information” instead of “big data”[20]. Such unnatural phrasing arises when software thesauruses common terms to evade plagiarism detection[20]. Our detector can include a dictionary of known tortured phrases (the Problematic Paper Screener community has compiled many of these). If the paper’s text contains multiple matches (e.g., “deep neural network” is oddly written as “profound neural organization”[21]), we raise a red flag. Additionally, purely AI-generated text often has a certain “style” – it may be overly fluent but nonspecific, use filler statements, or exhibit inconsistency in tone. GPT-4 itself can be harnessed to evaluate this: we can feed it sections of the paper (like the introduction or conclusion) and ask for signs of AI-generation. For instance, “Analyze the following text for signs it was generated by AI or is not scientifically sound.” GPT-4 might catch issues like repetitive structure, overly generic statements, or lack of domain-specific details. While GPT-4 is not a foolproof AI text detector, it can highlight obvious oddities. We can also use metrics like perplexity with a language model: AI-generated text often has lower perplexity to the model that generated it. OpenAI’s detectors are not very reliable, so a better use of GPT is qualitative analysis combined with known markers. In summary, linguistically we flag: presence of tortured/key phrases, excessive grammatical correctness paired with semantic nonsense, and sections that are inexplicably incoherent or on the contrary, too coherent in a generic way. These cues are quick to check – just scanning the text string for known bad phrases, or one-pass with GPT-4.
Fake Citation Detection: A hallmark of nonsense papers is fabricated or irrelevant citations. In the previous section we dealt with validating citations for a claim; those same techniques apply here globally to the paper. We will use the Semantic Scholar or OpenAlex lookup to verify all references in the bibliography. If many references don’t resolve to real papers, that’s a clear indicator of a fake paper (or at least a very poorly prepared one). Even if they exist, we assess their relevance: does the paper cite things that make sense for the topic? For example, if a biology paper’s reference list mostly contains computer science conference papers, that’s strange. We could use the vector of subject areas from OpenAlex for the references to see if they align with the claimed field of the paper. Another check is citation quality: are many citations from predatory or low-quality venues? If a paper cites a bunch of dubious journals, it might indicate a paper-mill product. We can cross-check each referenced journal against known lists of predatory journals (if available quickly) or see if none of the top-tier journals in the field are cited. Also, as mentioned, retracted citations are a red flag – if the bibliography is littered with retracted papers, the author might have just grabbed references blindly[22]. Using Semantic Scholar’s corpus or CrossRef, we can identify if any reference is flagged as retracted or has titles like “Retracted: …”. Our citation validation pipeline from part 4 thus directly feeds into the BS detector: a paper that fails the citation checks (many missing or irrelevant citations) will score high on the BS scale. Another specific trick: sometimes AI-generated texts include citations that look real but are slightly off (wrong year, minor title error). Verifying each reference’s metadata against what’s written in the paper can uncover those inconsistencies (though doing that fully automatically might be hard in short time). At least verifying existence and basic metadata (title similarity) can be done.
Metadata Inconsistencies: These are checks on the paper’s own metadata and content for logical consistency. For example, author info – do the authors’ affiliations make sense? If the paper claims all authors are from “Institute of Advanced Research, New York” but no such institute exists, that’s a sign. Or if the affiliations are strangely diverse for a small paper (e.g., one author from a random private company, one from an unrelated university, and none have emails or ORCIDs), it could be a fake. Another clue is journal metadata vs content: e.g., the PDF might say “International Journal of Computer Science” but the supposed content is about medicine. Paper mills sometimes slot content into random journal templates. We might check if the journal mentioned in the PDF (if any) actually exists and matches the topic. Moreover, genuine papers usually have certain sections (Methods, Results, etc., in research articles). If the structure is odd (like it reads more like an essay), that’s suspicious. We can use a heuristic: count section headings or look for expected keywords (an academic paper often has at least an Introduction, Conclusion, References section). If those are missing or mislabeled, the paper might not be a standard peer-reviewed article. Figures and Tables: many fake papers reuse or randomly generate figures. If all figures are oddly low-quality or irrelevant, it’s hard to automatically detect, but one could do an image search or detect if captions are nonsensical. For MVP, we likely skip deep image analysis.
Heuristics vs. Fine-tuning: Given the hackathon timeframe, we recommend a heuristic + LLM hybrid approach rather than trying to train a custom model. Fine-tuning a classifier (for example, training a model on known fake vs genuine papers) would require gathering a labeled dataset (which is non-trivial – though some exist, like the SCIGen papers or known paper-mill outputs) and then training and validating the model. This is time-consuming and risky (the model might not be accurate without lots of data). In contrast, heuristic rules (like keyword spotting and reference checks) can be implemented quickly and tend to catch the low-hanging fruit of BS detection. We can complement these with GPT-4’s reasoning for edge cases. For instance, use GPT-4 to analyze the paper’s abstract and ask it, “Does this abstract sound coherent and credible?” If GPT-4 answers that it’s full of jargon but says nothing, that’s a sign. A hybrid approach could be: run a series of checks (tortured phrases count, missing references count, etc.) to score the paper, and then feed a summary of these findings to GPT-4 asking, “Would you consider this paper likely fake?” This way, GPT-4’s assessment is guided by our heuristic findings.
Known Patterns to Flag: From literature on paper mill detection, some patterns we can implement include:
- Tortured phrases: e.g., check for a list of known misphrased terms[20].
- Generic statements: if the introduction is extremely generic (e.g., defines basic concepts at length or uses identical sentences to many other papers), it could be AI-generated. We could check uniqueness by searching a sentence on the web (if allowed) to see if it’s copied.
- Authorship anomalies: all authors from different fields or using private email domains (like Gmail) instead of institutional – not always fake, but often in suspect papers authors use non-institutional emails[23].
- Lack of specifics: e.g., a biology paper that never names a specific gene or chemical, using only generic terms. That might be AI output that avoids specifics it doesn’t know. GPT detection might catch that (“the text stays vague and lacks domain-specific details”).
- Excessive self-citation or mutual citation: As noted, if the paper heavily cites itself or a small group of authors’ work (beyond normal levels), it could indicate a coordinated effort or low novelty. We can quantify self-citations by matching author names between references and the paper.
- Publication venue check: If the paper claims to be published somewhere, verify that. Many fake papers will either not specify or use a fake conference/journal name. A quick web search or checking OpenAlex for the journal can confirm if it’s real.
By combining these factors, the module can output a “suspicion score” or simply a binary flag if multiple red flags are present. For example, a paper might be flagged because: it has tortured phrases[22], cites nonexistent articles, and has a mix of unrelated author affiliations – all together strongly suggest it’s generated nonsense. In a demo, we could visually mark such papers (e.g., highlight them in red on the graph, or present a warning icon next to their title).
Notably, this approach is heuristic, which is acceptable for an MVP. It may not catch all AI-generated papers, but it will catch the obvious ones. Academic news in recent years shows that many fake papers are quite crude, featuring easily detectable issues like the ones above[22][24]. Using GPT-4 as an on-demand expert to analyze text gives us additional adaptability: instead of training a model, we leverage GPT-4’s general knowledge of what a scientific paper should look like and what known scams look like. GPT-4 is aware of phenomena like “tortured phrases” and can recognize when text is strangely worded or content-free. This saves us from developing a custom ML model under time pressure.
In conclusion, a hybrid heuristic + GPT-based bullshit detector is recommended. Implement simple checks for language (e.g., string matching for known bad phrases), for citations (via the validation pipeline results), and for metadata oddities, then optionally have GPT-4 consider the paper with these highlights to make a final judgement. This approach will be far more feasible in a hackathon than collecting training data and fine-tuning a detector. And importantly, it will create a high-impact demo: you can show a paper node turning red because, say, “it contains tortured phrases and 30% of its references are unverified,” which is immediately understandable to users. By prioritizing these detectable anomalies[22][24], we maximize the chances of catching “AI-generated or suspicious” papers with minimal implementation effort, aligning with the hackathon’s fast pace and the project’s integrity goals.
Sources: The design and recommendations above are informed by recent research and articles on scientific integrity. For instance, common features of fraudulent papers (tortured phrases, irrelevant citations, etc.) have been documented in the literature[22]. These guide our heuristic rules. The advantages of JSON output for LLMs[1] and the capabilities of graph libraries[4][6] are drawn from benchmarks and library documentation. Using OpenAlex’s open data (with its high rate limits and retraction flags[18]) ensures our system can scale and stay up-to-date. By combining these insights, the ClaimGraph system can be built rapidly while still delivering a compelling live demo with interactive claim verification and credibility analysis.
________________________________________
[1] [2]  Enhancing structured data generation with GPT-4o evaluating prompt efficiency across prompt styles - PMC 
https://pmc.ncbi.nlm.nih.gov/articles/PMC11979239/
[3] [5] Knowledge graph visualization: A comprehensive guide [with examples]
https://datavid.com/blog/knowledge-graph-visualization
[4] [6] [7] Top 10 JavaScript Libraries for Knowledge Graph Visualization
https://www.getfocal.co/post/top-10-javascript-libraries-for-knowledge-graph-visualization
[8] A Look At Graph Visualization With Sigma React - William Lyon
https://lyonwj.com/blog/sigma-react-graph-visualization
[9] Paper Data - Semantic Scholar Academic Graph API
https://api.semanticscholar.org/api-docs/
[10] Semantic Scholar - APIs for Scholarly Resources - Research Guides
https://libguides.ucalgary.ca/c.php?g=732144&p=5260798
[11] Semantic Scholar Academic Graph API
https://www.semanticscholar.org/product/api
[12] Downloading large number of PDFs - Google Groups
https://groups.google.com/g/arxiv-api/c/WS7hR2A0OBM
[13] Release Plan for E-utility API Keys - NCBI Insights - NIH
https://ncbiinsights.ncbi.nlm.nih.gov/2018/08/14/release-plan-for-e-utility-api-keys/
[14] FAQ | OpenAlex technical documentation
https://docs.openalex.org/additional-help/faq
[15] [16] [17] Overview | OpenAlex technical documentation
https://docs.openalex.org/
[18] Work object | OpenAlex technical documentation
https://docs.openalex.org/api-entities/works/work-object
[19] How citation rings infiltrate peer review | Babak Heydari posted on ...
https://www.linkedin.com/posts/babak-heydari-963325a_i-always-thought-that-those-inexplicably-activity-7392790789766029312-7y3Y
[20] [21] ‘Tortured phrases’ give away fabricated research papers
https://www.nature.com/articles/d41586-021-02134-0?error=cookies_not_supported&code=f1e31559-da50-4f44-b32f-409a86ea0176
[22] [23] [24]  Combating Fake Science in the Age of Generative Artificial Intelligence: A Biomedical Perspective - PMC 
https://pmc.ncbi.nlm.nih.gov/articles/PMC12309808/

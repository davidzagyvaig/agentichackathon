ClaimGraph: The Trust Layer for Scientific Discovery — Architectural Blueprint & Implementation Strategy1. Executive Summary: The Epistemological Shift from Summarization to ProvenanceThe trajectory of artificial intelligence in scientific discovery has largely been defined by a retrieval-summarization paradigm. Tools such as Perplexity, Elicit, and Consensus have democratized access to scientific literature, functioning essentially as highly efficient search engines that aggregate and synthesize vast quantities of text. While these systems accelerate the rate at which researchers can consume information, they suffer from a critical epistemological flaw: they treat the retrieved text as ground truth. In the current landscape of scientific publishing, this assumption is increasingly dangerous. The proliferation of "paper mills," the rise of AI-generated "zombie science," and the persistence of citation rings have introduced a level of noise and fabrication that standard summarization models are ill-equipped to handle. A summarizer, by definition, amplifies the signal of the text it processes; if that text is fraudulent or hallucinated, the AI becomes a vector for misinformation.1ClaimGraph represents a fundamental departure from this model. It is not an Information Retrieval (IR) system, but an Evidence Verification (EV) engine. The core philosophy driving this architecture is that scientific truth is not found in the summary of an abstract, but in the rigorous traceability of claims to their underlying evidence. This system is designed to operate as a forensic auditor of scientific literature. Rather than summarizing papers, it extracts atomic claims, traces their citations recursively to primary sources, and subjects the entire evidence chain to topological and semantic stress tests.For the Umrand A1 Science First Hackathon, where the evaluation criteria prioritize "radical innovation" and the ability to "enable impossible science," ClaimGraph offers a distinct competitive advantage.1 It does not merely make research faster; it makes it trustworthy. By leveraging the available High-Performance Computing (HPC) cluster—specifically the 192GB of VRAM provided by dual NVIDIA RTX A6000 GPUs—the system performs local, high-fidelity linguistic analysis and graph computations that cloud-based API wrappers cannot replicate due to cost and latency constraints. This report outlines a comprehensive architectural blueprint for building ClaimGraph within the 1.5-day timeline, detailing the transition from a passive consumer of text to an active verifier of scientific claims.2. The Epistemological Crisis in AI-Driven Science2.1 The Failure of Retrieval-Augmented Generation (RAG)Standard Retrieval-Augmented Generation (RAG) architectures operate on a presumption of validity. When a user queries a system like ChatGPT or Elicit, the model retrieves the top-$k$ documents based on vector similarity and synthesizes an answer. This process mimics the literature review phase of research but skips the critical step of critique. A human scientist does not simply read an abstract and accept its conclusions; they verify the methodology, check the citations to ensure they support the claims made, and evaluate the author's track record.Current AI tools fail to distinguish between:Robust Science: Claims supported by reproducible experiments and valid citations."Zombie" Science: Claims based on retracted papers, non-existent studies, or circular reasoning.AI-Generated Fabrications: Papers generated by LLMs (e.g., using SCIgen or ChatGPT) that contain "tortured phrases" and hallucinated references to bypass plagiarism detection.2The consequence is a pollution of the scientific record. As noted in the hackathon context, "LLMs only recover 38.8% of ground-truth hypotheses," highlighting a severe gap in reliability.2 ClaimGraph addresses this by inserting a verification layer before any synthesis occurs.2.2 Competitive Landscape AnalysisThe hackathon environment provides access to several sophisticated tools, yet a granular analysis reveals that none address the problem of verification.Denario: Focuses on "Maker-Hater" debates for hypothesis generation and checks for novelty against Semantic Scholar. While it simulates scientific debate, it does not verify the veracity of the literature it ingests, leaving it vulnerable to garbage-in, garbage-out dynamics.1AstroAgents: A multi-agent system for hypothesis generation from mass spectrometry data. It excels at data interpretation but lacks a mechanism to verify the external literature it references for context.1Sakana AI Scientist: Automates the entire research loop from idea to paper. However, it operates on the assumption that the "related work" it cites is valid. It does not perform forensic analysis on the citation graph.1DATAGEN & OpenAI Hypothesis: These are generative tools focused on creating new ideas. They are downstream of the problem ClaimGraph solves. You cannot generate valid hypotheses if your foundational knowledge base is corrupted.1ClaimGraph's Unique Value Proposition: We are building the Trust Layer. By verifying the provenance of every claim, detecting "bullshit" (fake citations, tortured phrases), and visualizing the integrity of the evidence chain, ClaimGraph provides the necessary foundation for all other AI scientific tools. It is the "BS detector" that the scientific community desperately needs.3. Recommended Technical ArchitectureGiven the constraints of a 1.5-day timeline and the immense advantage of the HPC cluster (2x NVIDIA RTX A6000, 192GB VRAM), the architecture must prioritize local compute over external APIs wherever possible to maximize throughput and avoid rate limits.3.1 The High-Performance StackThe recommended stack maximizes the use of Python for heavy lifting (backend) while leveraging the React ecosystem for complex graph visualization.Frontend: Next.js 14 (App Router) combined with React Force Graph for 3D visualization. This library (a wrapper around Three.js and d3-force) allows for rendering thousands of nodes (claims and papers) with high performance, essential for visualizing complex citation networks.3Backend Orchestration: Python (FastAPI) serving as the central nervous system. It orchestrates the multi-agent workflow using LangGraph. Unlike purely linear chains, LangGraph supports cyclic workflows, which are crucial for verification loops (e.g., Check citation -> Failed -> Retry with fuzzy match -> Still failed -> Flag as hallucination).5Graph Database: Neo4j is the superior choice for this specific use case over PostgreSQL. While Postgres (with recursive CTEs) can handle simple hierarchies, Neo4j's native graph storage offers index-free adjacency, making the traversal of deep citation chains and the detection of circular references ($A \to B \to C \to A$) significantly faster and more intuitive to query using Cypher.7 We will deploy Neo4j via Docker on the HPC cluster.Vector & Relational Store: Supabase (PostgreSQL + pgvector). While Neo4j handles the topology, Supabase is ideal for storing user sessions, caching raw paper metadata, and handling vector embeddings for semantic search (hybrid search).9HPC Model Serving (The Engine):GPU 1 (A6000 - 48GB): Dedicated to vLLM serving Qwen 2.5-72B-Instruct (quantized to 4-bit or 8-bit). vLLM is critical here for its PagedAttention, which allows high-throughput concurrent inference for extracting claims from dozens of papers simultaneously.GPU 2 (A6000 - 48GB): Dedicated to Fine-Tuning and PDF Parsing. We will run Marker for high-fidelity PDF extraction and Unsloth for fine-tuning smaller expert models (e.g., Qwen 2.5-32B) on specific tasks like "Tortured Phrase" detection.113.2 System Architecture DiagramThe system follows a pipeline architecture where data flows from ingestion to verification to visualization.Code snippetgraph TD
    User[User Query] --> Frontend[Next.js Frontend]
    Frontend --> API[FastAPI Orchestrator]
    
    subgraph "Agentic Layer (LangGraph)"
        API --> SearchAgent
        SearchAgent --> FetchAgent[Fetch & Parse Agent]
        FetchAgent --> ExtractAgent[Claim Extraction Agent]
        ExtractAgent --> VerifyAgent[Verification Agent]
        VerifyAgent --> GraphBuilder
    end
    
    subgraph "HPC Cluster (192GB VRAM)"
        ExtractAgent -- "Inference Request" --> vLLM
        FetchAgent -- "PDF Processing" --> Marker[Marker GPU Parser]
        VerifyAgent -- "Classification" --> FineTuned
    end
    
    subgraph "Data Persistence"
        GraphBuilder --> Neo4j
        SearchAgent --> Supabase
    end
    
    subgraph "External Knowledge"
        SearchAgent --> SemSchol
        VerifyAgent --> Crossref[Crossref API]
    end
    
    GraphBuilder --> Serializer
    Serializer --> Frontend
4. Data Ingestion: Breaking the PDF BarrierThe first challenge in verifying scientific literature is accurately ingesting it. Scientific papers are notorious for complex layouts, two-column formats, mathematical equations, and floating figures. Traditional OCR tools (Tesseract) or simple text extractors (PyPDF2) fail catastrophically here, rendering the text unusable for precise claim extraction.4.1 The Paper Source: Semantic Scholar APIFor the hackathon, Semantic Scholar is the optimal data source. Unlike arXiv (preprints only) or PubMed (biomedical only), Semantic Scholar covers all domains and provides a well-structured Graph API.Rate Limits: With an API key (which we should request immediately or simulate via careful throttling), we get 1 request per second. The graph endpoints (/graph/v1/paper/{paper_id}/citations) are essential for retrieving the citation network without parsing the bibliography manually.13Full-Text Access: Semantic Scholar identifies Open Access PDFs. We will prioritize papers with available open-access links to demonstrate the full verification pipeline.Backup: OpenAlex is a robust alternative with a completely open catalog and generous rate limits (100k requests/day), which serves as an excellent fallback if Semantic Scholar throttles us.154.2 PDF Parsing with MarkerWe will utilize Marker, a specialized PDF-to-Markdown converter optimized for scientific texts. Marker outperforms traditional tools by using a pipeline of deep learning models to detect layout, extract equations (converting them to LaTeX), and handle tables cleanly.Why Marker? It benchmarks significantly higher (96% accuracy) than Nougat or Grobid on scientific datasets. Crucially, it is efficient on GPUs, allowing us to parse a paper in seconds on the A6000.16Implementation: The Fetch Agent will download the PDF stream, pass it to the Marker instance running on GPU 2, and receive clean, structured Markdown. This Markdown preserves the citation markers (e.g., ""), which is vital for the next step: mapping claims to citations.5. The "Bullshit Detector" Engine: Verification PipelineThis pipeline is the core differentiator of ClaimGraph. It assumes the role of a skeptical peer reviewer, actively looking for reasons to distrust the paper.5.1 Step 1: Atomic Claim ExtractionWe must move beyond "summaries" to "atomic claims." A summary aggregates information; an atomic claim isolates a specific assertion of fact (e.g., "Compound X inhibits protein Y with a roughly 50% efficacy").Model: Qwen 2.5-72B-Instruct served via vLLM. Qwen 2.5 has demonstrated state-of-the-art performance in coding and structured output generation, often outperforming Llama 3.1 in following complex JSON schemas.18Prompt Engineering: We will use a rigorous system prompt to enforce a strict JSON schema. The prompt must instruct the model to identify the claim and the specific citation marker associated with it.Input: Clean Markdown from Marker.Task: Extract list of {claim_text, claim_type, citation_marker, confidence}.Constraint: "Do not hallucinate citations. If a claim has no citation, mark as 'UNSUPPORTED'."5.2 Step 2: "Tortured Phrase" DetectionAI-generated papers (particularly those from paper mills using "spinbots" to evade plagiarism detection) often contain "tortured phrases"—strange synonyms for standard scientific terms. For example, "artificial intelligence" becomes "counterfeit consciousness," or "breast cancer" becomes "bosom peril".2Heuristic Database: We will integrate the dataset from the Problematic Paper Screener (Cabanac et al.), which lists thousands of known tortured phrases.21Algorithm: A fast, regex-based scanner (Aho-Corasick algorithm for speed) checks the extracted text against this dictionary.Trust Scoring: Detecting >3 unique tortured phrases immediately flags the paper as Critical Risk (Red Node) and halts further processing to save compute. This is a high-confidence signal of fraud.5.3 Step 3: Citation Integrity VerificationThis step verifies if the cited evidence actually exists and supports the claim.Existence Check: The system parses the extracted citation marker (e.g., "") and resolves it to the bibliography entry. It then queries the Semantic Scholar Graph API. Does the DOI exist? Is the paper retracted? (Using Crossref or Retraction Watch data).Semantic Support Check: This is the most advanced step. We retrieve the abstract of the cited paper. We then use a Natural Language Inference (NLI) model to classify the relationship between the Claim and the Cited Abstract.Model: A smaller, fine-tuned model (e.g., RoBERTa-large or Qwen 2.5-7B) fine-tuned on the SciFact dataset.22 SciFact is specifically designed for scientific claim verification with labels: SUPPORT, CONTRADICT, NEUTRAL.Logic:If Label == SUPPORT: Green Edge (Verified).If Label == CONTRADICT: Red Edge (Contradicted).If Label == NEUTRAL or Cited Paper is irrelevant: Yellow Edge (Weak Link).5.4 Step 4: Graph Topology Analysis (The "Cartel" Check)Citation rings (cartels) are groups of authors or journals that cite each other excessively to game impact factors.Cycle Detection: Using Neo4j's Graph Data Science (GDS) library or NetworkX, we run cycle detection algorithms to find tight loops ($A \to B \to A$) or cliques where citation density is suspiciously high compared to the domain average.23Self-Citation Ratio: We calculate the ratio of self-citations for the authors. A ratio exceeding 20-30% triggers a "Suspicious Author" flag.6. HPC Cluster Strategy: Fine-Tuning & InferenceThe availability of 192GB VRAM is a strategic asset. It allows us to run models locally that are too large for standard consumer hardware and too expensive for heavy API usage.6.1 Fine-Tuning Strategy: The "Claim Specialist"We will fine-tune a model specifically for the task of Scientific Claim Extraction and Mapping. General-purpose models often summarize rather than extract exact claims.Base Model: Qwen 2.5-32B-Instruct. This model sits in the "Goldilocks zone"—powerful enough for complex reasoning but small enough to fine-tune efficiently on an A6000.18Dataset: We will use the SciFact dataset, specifically the claims_train.jsonl file.22 This dataset contains expert-annotated scientific claims paired with evidence sentences.Framework: Unsloth. Unsloth allows for fine-tuning Llama and Qwen models 2x faster with 70% less memory usage compared to standard Hugging Face implementations.11 This enables us to fine-tune a 32B parameter model on a single 48GB GPU, or potentially even the 72B model distributed across both.Objective: Train the model to input a paragraph of scientific text and output a JSON object containing the claims and the exact span of text (evidence) that supports them.6.2 Local Inference Strategy: vLLMWe will deploy the fine-tuned model (or the base Qwen 2.5-72B) using vLLM on the second GPU.Configuration: We will use vLLM's tensor parallelism to split the 72B model across the GPUs if necessary, or run the quantized (GPTQ/AWQ) version on a single card.Optimization: Enable prefix caching in vLLM. Since the system prompt ("You are a scientific claim extractor...") remains constant, prefix caching will significantly reduce latency for subsequent requests.7. Knowledge Graph Schema & StorageA robust schema is essential for traversing the provenance of claims. We will use a property graph model in Neo4j.7.1 NodesPaper: {id: DOI, title: String, year: Int, trust_score: Float, status: Enum(Verified, Suspicious, Retracted)}Author: {id: ORCID/SemScholID, name: String, h_index: Int, self_citation_rate: Float}Claim: {id: UUID, text: String, topic: String, confidence: Float}Evidence: {id: UUID, snippet: String, type: Enum(Citation, Figure, Data)}Entity: {id: String, name: String, type: Enum(Chemical, Protein, Method, etc.)} (extracted via NER)7.2 Relationships(:Paper)-->(:Author)(:Paper)-->(:Claim)(:Claim)-->(:Evidence)(:Evidence)-->(:Paper) (This is the citation link)(:Paper)-->(:Paper) (The graph structure for cartel detection)(:Claim)-->(:Claim) (Inferred relationships between papers)7.3 Storage StrategyGraph Structure: Stored in Neo4j for traversal efficiency.Metadata & Content: Stored in Supabase. The full text of papers (JSON/Markdown) is cached here to prevent re-fetching.Embeddings: Claims are embedded using SPECTER2 (specialized for scientific text) and stored in pgvector within Supabase. This allows us to find semantically similar claims across different papers to detect consensus or conflict.268. Multi-Agent Orchestration (LangGraph)We will implement a stateful multi-agent system using LangGraph. LangGraph is superior to CrewAI for this application because it treats the workflow as a graph, allowing for conditional branching (e.g., "If verification fails, try alternative search") and loops, which are difficult to implement in linear agent chains.58.1 The Agent TeamOrchestrator: Manages the global state (ResearchState). Routes tasks to sub-agents.Librarian (Search & Fetch): Interfaces with Semantic Scholar and arXiv. Handles rate limits and pagination. Downloads PDFs.Analyst (Extraction): Sends PDFs to Marker (HPC) and then to the Qwen 2.5 Inference Server. Parses the JSON output.Auditor (Verification): The "Critic." Checks citation validity, runs the tortured phrases regex, and queries the NLI model for support verification. Updates the trust_score.Cartographer (Graph Builder): Ingests the verified data into Neo4j. Runs topological analysis (cycle detection) to identify citation rings.8.2 Workflow LogicPython# Simplified LangGraph Logic
def research_workflow(state: ResearchState):
    # Step 1: Search
    papers = librarian_agent.search(state.query)
    
    # Step 2: Parallel Processing Loop
    for paper in papers:
        # Extraction
        content = analyst_agent.process(paper)
        claims = analyst_agent.extract_claims(content)
        
        # Verification (The Core Loop)
        for claim in claims:
            verification = auditor_agent.verify(claim)
            if verification.status == "SUSPICIOUS":
                paper.trust_score -= 0.2
            elif verification.status == "HALLUCINATED":
                paper.flag("RED_NODE")
                break # Stop processing this paper
        
        # Graph Construction
        cartographer_agent.update_graph(paper, claims)
        
    return state
9. Deep Research Handoff & VisualizationThe final output is not just a graph but a synthesized report. However, unlike standard tools, our report is built only from verified nodes.9.1 Frontend VisualizationWe will use React Force Graph to render the provenance network.Visual Encoding:Nodes: Papers (Spheres), Claims (Cubes).Colors:Green: High Trust Score (>0.8). Verified citation chain.Yellow: Moderate Trust (0.5-0.8). Some broken links or unverified claims.Red: Low Trust (<0.5). Contains tortured phrases, retracted citations, or circular logic.Interactivity: Clicking a "Red Node" expands a sidebar showing the evidence of fraud (e.g., "This paper cites a retracted study" or "Contains the phrase 'counterfeit consciousness'").9.2 Deep Research SerializerTo generate the final report, we serialize the knowledge graph into a structured text format (JSON-LD or formatted XML) that fits into the context window of a large model (e.g., Gemini 1.5 Pro).Prompt Template:"You are a Principal Investigator. I have provided a serialized Knowledge Graph of verified scientific claims regarding '{topic}'.Data Structure:Verified Claims: Claims where the citation chain is intact and semantically supportive.Refuted/Suspicious Claims: Claims derived from 'Red Nodes' (papers with tortured phrases or broken citations).Task:Write a comprehensive deep research report.Synthesize the Verified Claims into a cohesive narrative.Explicitly highlight areas of Scientific Consensus vs. Conflict.Dedicate a section to Data Integrity: Explicitly mention any papers that were excluded or flagged as 'Red Nodes' and explain why (e.g., 'Paper X was flagged due to circular citation patterns').Do NOT hallucinate. Only use the provided Claims."10. Implementation Plan: The 1.5 Day SprintPhase 1: Infrastructure & Data (Hours 0-6)HPC Setup: Deploy vLLM with Qwen 2.5-72B (or 32B) on GPU 1. Deploy Marker on GPU 2.Database: Spin up Neo4j and Supabase (Docker).Data Prep: Download the SciFact dataset and the Tortured Phrases dictionary.Phase 2: The Engine (Hours 6-18)Fine-Tuning: Start the Unsloth fine-tuning job on GPU 2 (SciFact + Qwen 2.5-32B). This will take ~4-6 hours.Agent Code: Develop the Librarian and Analyst agents in LangGraph. Implement the connection to Semantic Scholar.Parser Integration: Connect the Analyst agent to the local Marker API.Phase 3: Verification & Graph (Hours 18-30)The Auditor: Implement the regex check for tortured phrases. Implement the citation check loop.Graph Builder: Write the Cypher queries to ingest nodes/edges into Neo4j.Frontend: Build the Next.js UI with React Force Graph. Connect to the Neo4j backend.Phase 4: Integration & Polish (Hours 30-36)Integration: Connect the frontend to the backend. Ensure the "Deep Research" button triggers the serialization and synthesis.Demo Prep: Pre-load a specific research topic (e.g., "Room Temperature Superconductors") where there is known "bullshit" (e.g., LK-99 analyses) to demonstrate the Red Node detection live.11. ConclusionClaimGraph transforms the role of AI in science from a passive summarizer to an active auditor. By combining the raw power of local HPC fine-tuning with a rigorous, graph-based verification architecture, we create a system that doesn't just read science—it defends it. In an age of information abundance, the ultimate scarcity is trust. ClaimGraph provides that trust.Project Name Recommendation: GroundTruth. It is strong, descriptive, and perfectly encapsulates the core philosophy of the system: tracing every claim back to the ground truth of evidence.